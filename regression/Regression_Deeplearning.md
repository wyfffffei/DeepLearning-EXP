# Regression And DeepLearning

> @ wyfffffei
> 
> ç¤ºä¾‹æ•°æ®é›†ï¼š<https://www.kaggle.com/harlfoxem/housesalesprediction>

## Data Init

### æ•°æ®æ£€æŸ¥

```python
import pandas as pd
import numpy as np
data = pd.read_csv("xxx.csv")
pd.options.display.max_columns = 25

# -> (æ•°æ®æ€»æ•°, æ¯ç¬”æ•°æ®åŒ…å«çš„ä¿¡æ¯æ•°)
print(data.shape)

# é»˜è®¤è¾“å‡ºå‰äº”æ¡
print(data.head())

# æ£€æŸ¥æ•°æ®ç±»å‹
print(data.dtypes)
```

### æ•°æ®é¢„å¤„ç†

```python
# æ•°æ®æ‹†åˆ†: date -> year + month
data["year"] = pd.to_numeric(data["date"].str.slice(0, 4))
data["month"] = pd.to_numeric(data["date"]).str.slice(4, 6)

# åˆ é™¤å¤šä½™æ•°æ® ('inplace'æŒ‡åˆ é™¤æºæ•°æ®)
data.drop(["id"], axis="columns", inplace=True)
data.drop(["date"], axis="columns", inplace=True)

# æ•°æ®åˆ†å‰²
# è·å–æ•°æ®ç´¢å¼•å¹¶æ‰“ä¹±
data_num = data.shape[0]
indexs = np.random.permutation(data_num)

# åˆ’åˆ†ä¸‰ç±»æ•°æ®é›†çš„ç´¢å¼• (0.6 + 0.2 + 0.2)
train_indexs = indexs[:int(data_num * 0.6)]
val_indexs = indexs[int(data_num * 0.6) : int(data_num * 0.8)]
test_indexs = indexs[int(data_num * 0.8):]

# é€šè¿‡ç´¢å¼•å–å‡ºæ•°æ®
train_data = data.loc[train_indexs]
val_data = data.loc[val_indexs]
test_data = data.loc[test_indexs]
```

### å½’ä¸€åŒ–ï¼ˆNormalizationï¼‰

![feature-scaling.png](./img/feature-scaling.png)

```python
# Standard Score (z-score)
# mean: å¹³å‡å€¼
# std: æ ‡å‡†å·®
# x_norm = (x - mean) / std

train_validation_data = pd.concat([train_data, val_data])
mean = train_validation_data.mean()
std = train_validation_data.std()

train_data = (train_data - mean) / std
val_data = (val_data -mean) / std
```

### å»ºç«‹ Numpy æ ¼å¼æ•°æ®é›†

```python
# è®­ç»ƒè¾“å…¥å€¼å’Œé¢„è®¡è¾“å‡ºå€¼
x_train = np.array(train_data.drop("price", axis="columns"))
y_train = np.array(train_data["price"])

# éªŒè¯æ•°æ®é›†åŒä¸Š
# x_val = ...
# ...

# æŸ¥çœ‹è®­ç»ƒæ•°æ®é›†æ€»æ•°
print(x_train.shape)
```

## Model Create

```python
class FullConnection(nn.Module):
    def __init__(self):
        super(FullConnection, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(784, 200),
            nn.LeakyReLU(inplace=True),
            nn.Linear(200, 200),
            nn.LeakyReLU(inplace=True),
            nn.Linear(200, 10),
            nn.LeakyReLU(inplace=True)
        )

    def forward(self, x):
        x = self.model(x)
        return x

# func = FullConnection()
# input = torch.randn(784, requires_grad=True)
# out = func(input)
# print(out)

device = torch.device("cpu")
net = FullConnection().to(device)
learning_rate = 1e-2
epochs = 100
optimizer = optim.Adam(net.parameters(), lr=learning_rate)
loss_func = nn.CrossEntropyLoss().to(device)
train_data, test_data = dataloader()
logger = SummaryWriter("./logs_train")
```

## Train the Model (step)

```python
for epoch in range(epochs):
    # ä¸€è½®è®­ç»ƒ
    net.train()
    for batch_id, (tra_data, tra_target) in enumerate(train_data):
        tra_data, tra_target = tra_data.to(device), tra_target.to(device)
        pre_train = net(tra_data)
        loss = loss_func(pre_train, tra_target)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch_id % 10 == 0:
            print("Train Epoch : {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}".format(
                epoch, batch_id*len(tra_data), len(train_data), 100.*batch_id/len(train_data), loss.item()
            ))
            logger.add_scalar("train_loss", loss.item(), batch_id)
```

## Test the Model (step)

```python
net.eval()
test_loss = 0
accurancy = 0
with torch.no_grad():
    for tes_data, tes_target in test_data:
        tes_data, tes_target = tes_data.to(device), tes_target.to(device)
        pre_test = net(tes_data)
        test_loss += loss_func(pre_test, tes_target).item()
        accurancy += (pre_test.argmax(1) == tes_target).sum()
test_loss /= len(test_data
print("\nTest set : Average loss: {:.4f}, Accurancy: {}/{}({:.3f}%)".format(
    test_loss, accurancy, len(test_data), 100.*accurancy/len(test_data)
))
logger.add_scalar("test_loss", test_loss, epoch)
logger.add_scalar("test_accuracy", accurancy, epoch) 
```

## Better Training

### å˜åŒ–çš„å­¦ä¹ ç‡ï¼ˆAdaptive Learning Rateï¼‰

#### Adagrad

æ ¹æ®æ¢¯åº¦ï¼Œä¸æ–­è°ƒæ•´å­¦ä¹ ç‡ï¼ˆæ­¥é•¿ï¼‰ï¼Œå¤§æ¢¯åº¦èµ°å°æ­¥ï¼Œå°æ¢¯åº¦èµ°å¤§æ­¥

ä¼˜ç‚¹ï¼šé€‚åˆå¤„ç†ç¨€ç–æ•°æ®ï¼Œå¯ä»¥æ˜¾è‘—æé«˜SGDçš„ç¨³å¥æ€§ï¼Œä¸éœ€è¦æ‰‹åŠ¨è°ƒæ•´å­¦ä¹ ç‡

ç¼ºç‚¹ï¼šåœ¨åˆ†æ¯ä¸­ç´¯è®¡å¹³æ–¹æ¢¯åº¦ï¼Œä¼šå¯¼è‡´ early stopping ç°è±¡ï¼Œå³å­¦ä¹ ç‡è¿‡æ—©å˜å°ï¼Œå¯¼è‡´æ¨¡å‹è¿˜æœªæ”¶æ•›å°±åœæ»ä¸å‰

![adagrad.png](./img/adagrad.png)

```python
grad_squared = 0
while True:
    dx = compute_gradient(x)
    grad_squared += dx * dx
    x -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7)
```

#### RMSProp

é‡‡ç”¨æŒ‡æ•°åŠ æƒå¹³å‡çš„æ€æƒ³ï¼Œåªå°†æœ€è¿‘çš„æ¢¯åº¦ç´¯åŠ è®¡ç®—å¹³æ–¹ï¼Œè¿™é¿å…äº† early stopping ç°è±¡

![rmsprop.png](./img/rmsprop.png)

```python
grad_squared = 0
while True:
    dx = compute_gradient(x)
    grad_squared = decay_rate * grad_squared + (1 - decay_rate) * dx * dx
    x -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7)
```

#### Momentum

ä¼˜ç‚¹ï¼šé¿å…æ¢¯åº¦ä¸‹é™æ—¶åœæ»åœ¨ local minimaï¼Œå¢åŠ ä¸€ä¸ªåŸæ¥çš„æ–¹å‘çš„å†²é‡ï¼ˆæƒ¯æ€§ï¼‰

ç¼ºç‚¹ï¼šä¸èƒ½ä¿è¯çªç ´ local minima

![momentum.png](./img/momentum.png)

![momentum_2.png](./img/momentum_2.png)

#### Adam âœ”

RMSProp + Momentum

![adam.png](./img/adam.png)

### æ¢¯åº¦æ¶ˆå¤±ï¼ˆVanishing Gradient Problemï¼‰

*Qï¼šè®­ç»ƒæ—¶ï¼Œæ¢¯åº¦æ—¶å¤§æ—¶å°ï¼Œä¸ç¨³å®š*

*Aï¼šä½¿ç”¨æ¿€æ´»å‡½æ•°ï¼Œç»Ÿä¸€è¾“å‡º*

#### ReLU

![ReLU.png](./img/ReLU.png)

![ReLU_2.png](./img/ReLU_2.png)

#### Maxout

![Maxout.png](./img/Maxout.png)

## Better Testing

### è¿‡æ‹Ÿåˆï¼ˆoverfittingï¼‰

*Qï¼šè®­ç»ƒçš„ç½‘ç»œæ¨¡å‹å¯¹éªŒè¯æ•°æ®é›†æ€§èƒ½å¾ˆå·®ï¼ˆLosså…ˆé™åå‡ï¼‰ï¼Œä½†å¯¹è®­ç»ƒæ•°æ®é›†æ€§èƒ½å¾ˆå¥½*

*Aï¼šæ¨¡å‹å¤ªå¤æ‚æˆ–è€…è®­ç»ƒæ•°æ®å¤ªå°‘*

 ![overfitting.png](./img/overfitting.png)

#### ææ—©ç»“æŸï¼ˆEarly Stoppingï¼‰

> <https://keras.io/getting-started/faq/#how-can-i-interrupt-training-whenthe-validation-loss-isnt-decreasing-anymore>

#### ç¼©å‡æ¨¡å‹å¤§å°

é¿å…è¿‡æ‹Ÿåˆï¼Œä½†ä¹Ÿå¯èƒ½é€ æˆæ¨¡å‹è¿˜æœªæ”¶æ•›å®Œå…¨

![toodeep.png](./img/toodeep.png)

#### æƒé‡æ­£åˆ™åŒ–ï¼ˆWeights Regularizationï¼‰

åœ¨æ¨¡å‹çš„æ‹Ÿåˆè¿‡ç¨‹ä¸­ï¼Œæƒé‡è¶Šå°ï¼Œå‡½æ•°å¾€å¾€è¶Šå¹³æ»‘ï¼Œå› æ­¤åœ¨ loss function çš„åŸºç¡€ä¸Šåœ¨åŠ ä¸€ä¸ªå’Œæ‰€æœ‰æƒé‡ç›¸å…³çš„å€¼ï¼Œç›®çš„æ˜¯ä½¿æƒé‡ä¹Ÿå°½å¯èƒ½çš„å°ï¼Œä¸äº’ç›¸ä¾èµ–

![weight_regularization.png](./img/weight_regularization.png)

- L1 Regularizationï¼š$Loss_T = Loss_M + Î»Î£_j|w_j|$

- L2 Regularizationï¼š$Loss_T = Loss_M + Î»Î£_j|w_jÂ²|$

*w* ï¼šæ¨¡å‹çš„æƒé‡

*j* ï¼šä»£è¡¨æ¨¡å‹çš„æƒé‡æ•°é‡ï¼ˆ0 -> Mï¼‰

*$Loss_M$*ï¼šä»£è¡¨æŸå¤±å‡½æ•°ä¸ºå‡æ–¹è¯¯å·®

*$Loss_T$*ï¼šä¸ºâ€œæŸå¤±å€¼â€å’Œâ€œæ­£åˆ™åŒ–æƒé‡â€ä¹‹å’Œ

*$Î»$*ï¼šå¯è°ƒæ•´å‚æ•°ï¼Œç”¨æ¥æ§åˆ¶æƒé‡æ­£åˆ™åŒ–å¼ºåº¦ï¼Œé€‰å–æ–¹æ¡ˆ ğŸ‘‡

 ![weight_regularization_2.png](./img/weight_regularization_2.png)

![weight_regularization_3.png](./img/weight_regularization_3.png)

![weight_regularization_4.png](./img/weight_regularization_4.png)

#### Dropout

åœ¨æ¯æ¬¡è®­ç»ƒæ—¶ï¼Œéšæœºå¤±æ´»éƒ¨åˆ†å‚æ•°ï¼ˆä»…åœ¨è®­ç»ƒä¸­ä½¿ç”¨ï¼‰ï¼Œé¿å…ç¥ç»å…ƒè¿‡åˆ†ä¾èµ–å±€éƒ¨ç‰¹å¾

![dropout.png](./img/dropout.png)

![dropout_2.png](./img/dropout_2.png)
